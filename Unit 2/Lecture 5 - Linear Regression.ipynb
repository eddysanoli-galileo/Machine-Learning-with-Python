{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 5 - Linear Regression\n",
    "\n",
    "### 4. Empirical Risk\n",
    "\n",
    "-----\n",
    "\n",
    "#### Compute Hinge Loss\n",
    "\n",
    "The empirical risk $R_n$ is defined as\n",
    "\n",
    "$$R_ n(\\theta ) = \\frac{1}{n} \\sum _{t=1}^{n} \\text {Loss}(y^{(t)} - \\theta \\cdot x^{(t)})$$\n",
    "\n",
    "where $(x^{(t)}, y^{(t)})$ is the th training example (and there are $n$ in total), and ${Loss}$ is some loss function, such as hinge loss.\n",
    "\n",
    "Recall from a previous lecture that the definition of hinge loss:\n",
    "\n",
    "$$\\text {Loss}_ h(z) = \\begin{cases}  0 & \\text {if } z \\geq 1 \\\\ 1 -z, & \\text { otherwise} \\end{cases}$$\n",
    "\n",
    "In this problem, we calculate the empirical risk with hinge loss when given specific $\\theta$ and $\\big \\{ (x^{(t)}, y^{(t)})\\big \\} _{t=1,...,n}$. Assume we have  training examples (i.e. $n=4$), where $x^{(t)}\\in \\mathbb {R}^3$ and $y^{(t)}$ is a scalar. The training examples $\\big \\{ (x^{(t)}, y^{(t)})\\big \\} _{t=1,2,3,4}$ are given as follows:\n",
    "\n",
    "![4-1](Media/4-1.PNG)\n",
    "\n",
    "Also, we have $\\theta = \\big [ 0,1,2\\big ]^ T$. Compute the value of\n",
    "\n",
    "$$R_ n(\\theta ) = \\frac{1}{4} \\sum _{t=1}^{4} \\text {Loss}_ h(y^{(t)} - \\theta \\cdot x^{(t)}).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 1.25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X training examples\n",
    "xt : np.ndarray = np.asarray(\n",
    "    [\n",
    "        [ 1, 0,  1],\n",
    "        [ 1, 1,  1],\n",
    "        [ 1, 1, -1],\n",
    "        [-1, 1,  1]\n",
    "    ]\n",
    ").T\n",
    "\n",
    "# Y training examples\n",
    "yt : np.ndarray = np.asarray([[2, 2.7, -0.7, 2]])\n",
    "\n",
    "# Linear regression parameters\n",
    "theta : np.ndarray = np.asarray([[0, 1, 2]])\n",
    "\n",
    "# Hinge loss\n",
    "def hinge_loss(z : np.ndarray):\n",
    "\n",
    "    # Set loss as 1-Z unless a member of \"z\" is higher than 1\n",
    "    loss = 1 - z\n",
    "    loss[z >= 1] = 0\n",
    "    return loss\n",
    "\n",
    "# Number of samples for each training example\n",
    "n = xt.shape[1]\n",
    "\n",
    "# Empirical risk\n",
    "Rn : np.float16 = (1/n) * np.sum( hinge_loss(yt - np.dot(theta, xt)) )\n",
    "\n",
    "# Final answer\n",
    "print(\"Answer:\", Rn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Squared Error Loss\n",
    "\n",
    "Now, we will calculate the empirical risk with the squared error loss. Remember that the squared error loss is given by:\n",
    "\n",
    "$$\\displaystyle \\text {Loss}(z) = \\frac{z^2}{2}$$\n",
    "\n",
    "The 4 training examples and the $\\theta$ parameters are as in the previous problem. Compute the empirical risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 0.1475\n"
     ]
    }
   ],
   "source": [
    "# Squared error loss\n",
    "def squared_error_loss(z : np.ndarray):\n",
    "    return z**2 / 2\n",
    "\n",
    "# Empirical risk\n",
    "Rn : np.float16 = (1/n) * np.sum( squared_error_loss(yt - np.dot(theta, xt)) )\n",
    "\n",
    "# Final answer\n",
    "print(\"Answer:\", Rn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Risk and Model Performance\n",
    "\n",
    "![4-2](Media/4-2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "### 5. Gradient Based Approach\n",
    "\n",
    "The gradient for our previous problem can be calculated as follows:\n",
    "\n",
    "$$\\nabla_\\theta \\frac{(y^t - \\theta x^t)^2}{2} = (y^t - \\theta x^t) \\nabla_\\theta (y^t - \\theta x^t) = -(y^t - \\theta x^t) x^t$$\n",
    "\n",
    "Algorithm:\n",
    "- Initialize $\\theta = 0$\n",
    "- Randomly pick some example \"t\" from 1 to $n$\n",
    "- $\\theta = \\theta + \\alpha ((y^t - \\theta x^t) x^t) $\n",
    "\n",
    "Depending on our learning rate we can take a very big or small step in the opposite direction of the gradient. The learning rate may be a function of the number of iterations $K$ like so\n",
    "\n",
    "$$\\alpha_K = \\frac{1}{1+K}$$\n",
    "\n",
    "This setup allows us to make smaller and smaller steps as the algorithm goes longer and longer.\n",
    "\n",
    "Curiously this algorithm is self correcting. For example, if the prediction $\\theta x^t$ results in a value much smaller than $y^t$, their subtraction ($y^t - \\theta x^t$) will be positive, pushing the algorithm to add bigger values to theta and then reducing that gap between the prediction and ground truth.\n",
    "\n",
    "![4-3](Media/4-3.PNG)\n",
    "![4-4](Media/4-4.PNG)\n",
    "\n",
    "-----\n",
    "\n",
    "### 6. Closed Form Solution\n",
    "\n",
    "Notes:\n",
    "- Due to the previous loss being a convex function, it can be solved in a closed form.\n",
    "- After obtaining the gradient of the empirical risk, we get an expression of the form $A\\theta = b$. If $A$ is reversible, then we can get an exact solution for $\\theta$. However, $A$ can be reversible if the number of training samples is substantially larger than the dimensionality of the feature vector.\n",
    "\n",
    "$$R_ n(\\theta ) = \\frac{1}{n} \\sum _{t=1}^{n} \\frac{(y^{(t)} - \\theta \\cdot x^{(t)})^2}{2},$$\n",
    "\n",
    "$$\\displaystyle  \\displaystyle \\nabla R_ n(\\theta ) = A\\theta - b (=0) \\quad \\text {where } \\,  A = \\frac{1}{n} \\sum _{t=1}^{n} x^{(t)} ( x^{(t)})^ T,\\,  b = \\frac{1}{n} \\sum _{t=1}^{n} y^{(t)} x^{(t)}.$$\n",
    "\n",
    "- Sometimes getting the closed form solution can be computationally costly, as the complexity of the problem increases in squared time.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Generalization and Regularization\n",
    "### 8. Regularization\n",
    "\n",
    "Notes:\n",
    "- What happens if you dont have enough training samples? What happens if your training data has some noise? You use regularization.\n",
    "- Regularization: Pushes you away from trying to perfectly fit your training examples. Generally algorithms try to be lazy and set on parameters that are equal to 0. Here we apply a strong push to prevent this lazyness.\n",
    "- Now, we will add a new value to the empirical risk: The square norm of the parameters. \n",
    "- The relative contribution of the regularization is controlled by lambda\n",
    "- Rn tries to find thetas as good as possible. $||\\theta||^2$ will always consist of a positive number, so it tends to pull the value of $\\theta$ back to a base value. This means that noise will no longer affect the estimation, as only a very significant push will be able to escape the effect of the regularization value.\n",
    "\n",
    "$$J_{\\lambda, n}(\\theta) = \\frac{\\lambda}{2}||\\theta||^2 + R_n(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Al incrementar el valor de lambda, decimos que nos importa el \"empirical risk\", pero también nos importa mantener nuestro valor de Theta lo más pequeña posible. Ahora, mientras más alta sea la lambda, peor será nuestra predicción. Hacemos esto porque ya no queremos que cada pequeñito cambio en los datos de entrenamiento causen un cambio significativo en los parámetros, queremos que el modelo generalice mejor, entonces le introducimos un valor base alrededor del cual los parámetros se ubicarán, para que los mismos se muevan únicamente cuando haya evidencia sustancial que apoye el cambio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('Unit_2-U_ivV9B3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c349c5de5661e8b703439efd95ea8a79ed352523fcc82505d8958424d4a970b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
