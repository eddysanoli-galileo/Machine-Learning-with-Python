{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2: Digit Recognition\n",
    "\n",
    "### Setup\n",
    "\n",
    "1. Note on software: For all the projects, we will use python 3.6 augmented with the NumPy numerical toolbox, the matplotlib plotting toolbox. In this project, we will also use the scikit-learn package, which you could install in the same way you installed other packages, as described in project 0, e.g. by conda install scikit-learn or pip install sklearn\n",
    "\n",
    "2. Download mnist.tar.gz and untar it into a working directory. The archive contains the various data files in the Dataset directory, along with the following python files:\n",
    "\n",
    "   - `part1/linear_regression.py` where you will implement linear regression\n",
    "   - `part1/svm.py` where you will implement support vector machine\n",
    "   - `part1/softmax.py` where you will implement multinomial regression\n",
    "   - `part1/features.py` where you will implement principal component analysis (PCA) dimensionality reduction\n",
    "   - `part1/kernel.py` where you will implement polynomial and Gaussian RBF kernels\n",
    "   - `part1/main.py` where you will use the code you write for this part of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from mnist import utils as mnist_utils\n",
    "from typing import Tuple\n",
    "\n",
    "# Reload custom package\n",
    "import importlib\n",
    "importlib.reload(mnist_utils)\n",
    "\n",
    "# Get MNist data\n",
    "train_x, train_y, test_x, test_y = mnist_utils.get_MNIST_data()\n",
    "train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### 2. Linear Regression with Closed Form Solution\n",
    "\n",
    "#### Closed Form Solution of Linear Regression\n",
    "\n",
    "$$ \\displaystyle  \\displaystyle \\theta = (X^ T X + \\lambda I)^{-1} X^ T Y $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Percentage: 0.744\n"
     ]
    }
   ],
   "source": [
    "def closed_form(X, Y, lambda_factor):\n",
    "    \"\"\"\n",
    "    Computes the closed form solution of linear regression with L2 regularization\n",
    "\n",
    "    Args:\n",
    "        X - (n, d + 1) NumPy array (n datapoints each with d features plus the bias feature in the first dimension)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "    Returns:\n",
    "        theta - (d + 1, ) NumPy array containing the weights of linear regression. Note that theta[0]\n",
    "        represents the y-axis intercept of the model and therefore X[0] = 1\n",
    "    \"\"\"\n",
    "    # Number of training features\n",
    "    num_features = X.shape[1]\n",
    "\n",
    "    # Identity matrix\n",
    "    I = np.identity(num_features)\n",
    "\n",
    "    # Closed form solution\n",
    "    theta = np.dot( np.linalg.inv( np.dot(X.T, X) + lambda_factor * I ),  np.dot(X.T, Y) )\n",
    "    return theta\n",
    "\n",
    "# Regularization parameter\n",
    "lambda_factor = 0.01\n",
    "\n",
    "# Get the models parameters\n",
    "theta = closed_form(train_x, train_y, lambda_factor)\n",
    "\n",
    "# Get the predicted value for Y (X * Theta)\n",
    "test_y_pred = np.round(np.dot(test_x, theta))\n",
    "\n",
    "# Truncate values to a min of 0 and a max of 9\n",
    "test_y_pred[test_y_pred < 0] = 0\n",
    "test_y_pred[test_y_pred > 9] = 9\n",
    "\n",
    "# Get the percentage of wrong guesses\n",
    "# (100% - Sum of all the correct guesses divided by the total number of guesses or the mean)\n",
    "error = 1 - np.mean(test_y_pred == test_y)\n",
    "\n",
    "print(\"Error Percentage:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alice and you find that no matter what $\\lambda$ factor you try, the test error is large. With some thinking, you realize that something is wrong with this approach.\n",
    "\n",
    "- Answer: The loss function related to the closed form solution is inadequate for this problem.\n",
    "\n",
    "- Reason: The loss function used in regression is least square which does not work well for classification problems. Instead you should use logistic loss function for this task. Suppose you have a binary classification problem, where you want to label data points as +1 or -1. The way you typically set up a classification problem is as follows â€” for a given data point x, you calculate a function f(x) and if that function value is large enough, you classify that point as +1, otherwise as -1. Now, if you use one of the standard classification losses like hinge loss or logistic loss, then for large values of f(x), if your true label is +1, then these losses go to zero. That is, you do not want to penalize correctly labeled points, and therefore keep them away from interfering with your parameter updates. However, with mean squared error, if your f(x) is anything other than +1, you incur a loss, and even correctly classified points influence the gradients, making learning harder.\n",
    "\n",
    "------\n",
    "\n",
    "### 3. Support Vector Machine\n",
    "\n",
    "#### One vs. Rest SVM\n",
    "\n",
    "Use the sklearn package and build the SVM model on your local machine. Use random_state = 0, C=0.1 and default values for other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Percentage: 0.007499999999999951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def one_vs_rest_svm(train_x, train_y, test_x):\n",
    "    \"\"\"\n",
    "    Trains a linear SVM for binary classifciation\n",
    "\n",
    "    Args:\n",
    "        train_x - (n, d) NumPy array (n datapoints each with d features)\n",
    "        train_y - (n, ) NumPy array containing the labels (0 or 1) for each training data point\n",
    "        test_x - (m, d) NumPy array (m datapoints each with d features)\n",
    "    Returns:\n",
    "        pred_test_y - (m,) NumPy array containing the labels (0 or 1) for each test data point\n",
    "    \"\"\"\n",
    "\n",
    "    # Regularization parameter and the random state used for \n",
    "    # the pseudo random number generator inside the SVM\n",
    "    C = 0.1\n",
    "    random_state = 0\n",
    "\n",
    "    # Create the SVM model\n",
    "    svc = LinearSVC(C = C, random_state = random_state)\n",
    "    \n",
    "    # Train the model\n",
    "    svc.fit(train_x, train_y)\n",
    "\n",
    "    # Predict new data using the previous training\n",
    "    pred_test_y = svc.predict(test_x)\n",
    "\n",
    "    # Get the parameters for this estimator\n",
    "    params = [None, None]\n",
    "    params[0] = svc.coef_\n",
    "    params[1] = svc.intercept_\n",
    "\n",
    "    return pred_test_y, params\n",
    "\n",
    "\n",
    "# Now the classifiction will go into detecting if the digit is either not a zero (1)\n",
    "# or a zero (0). Since the labels in train_y come values from 0 to 9, all the values\n",
    "# from 1-9 are truncated to 1.\n",
    "train_y_one_vs_rest = train_y.copy()\n",
    "train_y_one_vs_rest[train_y_one_vs_rest > 0] = 1\n",
    "\n",
    "# Predict new data using the test data, while using the training data to generate a model\n",
    "y_pred, params = one_vs_rest_svm(train_x, train_y_one_vs_rest, test_x)\n",
    "\n",
    "# Change the classification labels for the test data as well\n",
    "test_y_one_vs_rest = test_y.copy()\n",
    "test_y_one_vs_rest[test_y_one_vs_rest > 0] = 1\n",
    "\n",
    "# Get the error percentage\n",
    "error = 1 - np.mean(y_pred == test_y_one_vs_rest)\n",
    "print(\"Error Percentage:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass SVM\n",
    "\n",
    "In fact, sklearn already implements a multiclass SVM with a one-vs-rest strategy. Use LinearSVC to build a multiclass SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Percentage: 0.08189999999999997\n"
     ]
    }
   ],
   "source": [
    "def multi_class_svm(train_x, train_y, test_x):\n",
    "    \"\"\"\n",
    "    Trains a linear SVM for multiclass classifciation using a one-vs-rest strategy\n",
    "\n",
    "    Args:\n",
    "        train_x - (n, d) NumPy array (n datapoints each with d features)\n",
    "        train_y - (n, ) NumPy array containing the labels (int) for each training data point\n",
    "        test_x - (m, d) NumPy array (m datapoints each with d features)\n",
    "    Returns:\n",
    "        pred_test_y - (m,) NumPy array containing the labels (int) for each test data point\n",
    "    \"\"\"\n",
    "\n",
    "    # Regularization parameter and the random state used for \n",
    "    # the pseudo random number generator inside the SVM\n",
    "    C = 0.1\n",
    "    random_state = 0\n",
    "\n",
    "    # Create the SVM model\n",
    "    svc = LinearSVC(C = C, random_state = random_state)\n",
    "    \n",
    "    # Train the model\n",
    "    svc.fit(train_x, train_y)\n",
    "\n",
    "    # Predict new data using the previous training\n",
    "    pred_test_y = svc.predict(test_x)\n",
    "    return pred_test_y\n",
    "\n",
    "\n",
    "# Predict new data using the test data, while using the training data to generate a model\n",
    "y_pred = multi_class_svm(train_x, train_y, test_x)\n",
    "\n",
    "# Get the error percentage\n",
    "error = 1 - np.mean(y_pred == test_y)\n",
    "print(\"Error Percentage:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### 4. Multinomial (Softmax) Regression and Gradient Descent\n",
    "\n",
    "#### Computing Probabilities for Softmax\n",
    "\n",
    "Write a function compute_probabilities that computes, for each data point $x^{(i)}$, the probability that $x^{(i)}$ is labeled as $j$ for $j = 0,1,\\dots ,k-1$.\n",
    "\n",
    "The softmax function $h$ for a particular vector $x$ requires computing\n",
    "\n",
    "$$h(x) = \\frac{1}{\\sum _{j=0}^{k-1} e^{\\theta _ j \\cdot x / \\tau }} \\begin{bmatrix}  e^{\\theta _0 \\cdot x / \\tau } \\\\ e^{\\theta _1 \\cdot x / \\tau } \\\\ \\vdots \\\\ e^{\\theta _{k-1} \\cdot x / \\tau } \\end{bmatrix},$$\n",
    "\n",
    "where $\\tau >0$ is the **temperature parameter**. When computing the output probabilities (they should always be in the range $[0,1]$), the terms $e^{\\theta _ j \\cdot x / \\tau }$ may be very large or very small, due to the use of the exponential function. This can cause numerical or overflow errors. To deal with this, we can simply subtract some fixed amount  from each exponent to keep the resulting number from getting too large. Since\n",
    "\n",
    "$$h(x) = \\frac{e^{-c}}{e^{-c}\\sum _{j=0}^{k-1} e^{\\theta _ j \\cdot x / \\tau }} \\begin{bmatrix}  e^{\\theta _0 \\cdot x / \\tau } \\\\ e^{\\theta _1 \\cdot x / \\tau } \\\\ \\vdots \\\\ e^{\\theta _{k-1} \\cdot x / \\tau } \\end{bmatrix} \\\\ = \\frac{1}{\\sum _{j=0}^{k-1} e^{[\\theta _ j \\cdot x / \\tau ] - c}} \\begin{bmatrix}  e^{[\\theta _0 \\cdot x / \\tau ] - c} \\\\ e^{[\\theta _1 \\cdot x / \\tau ] - c} \\\\ \\vdots \\\\ e^{[\\theta _{k-1} \\cdot x / \\tau ] - c} \\end{bmatrix}$$\n",
    " \n",
    "subtracting some fixed amount $c$ from each exponent will not change the final probabilities. A suitable choice for this fixed amount is $c = \\max _ j \\theta _ j \\cdot x / \\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: (2, 10)\n",
      "H: (2, 10)\n"
     ]
    }
   ],
   "source": [
    "def compute_probabilities(X, theta, temp_parameter):\n",
    "    \"\"\"\n",
    "    Computes, for each datapoint X[i], the probability that X[i] is labeled as j\n",
    "    for j = 0, 1, ..., k-1\n",
    "\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our model for label j\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "    Returns:\n",
    "        H - (k, n) NumPy array, where each entry H[j][i] is the probability that X[i] is labeled as j\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict the \"y's\" from the dataset using X and theta\n",
    "    Y = np.dot(theta, X.T)\n",
    "\n",
    "    # Apply the temperature parameter to the predictions\n",
    "    Y_temp = Y / temp_parameter\n",
    "\n",
    "    # Get the max value for the \"Y_temp\" exponent\n",
    "    c = np.max(Y_temp, axis=0)\n",
    "\n",
    "    # Get the exponentials for all samples\n",
    "    exponentials = np.exp( Y_temp - c )\n",
    "    \n",
    "    # Get the value for the softmax function\n",
    "    H = exponentials / np.sum(exponentials, axis = 0)\n",
    "    return H\n",
    "\n",
    "\n",
    "X = np.asarray([\n",
    "    [0.16293629, 0.05701168, 0.45172072, 0.36527799, 0.0583312 ],\n",
    "    [0.38338392, 0.84066432, 0.46417229, 0.46476983, 0.58996013],\n",
    "    [0.97103108, 0.18383185, 0.77164735, 0.07827019, 0.84502797],\n",
    "    [0.54848594, 0.35306614, 0.80574031, 0.43001758, 0.33883494],\n",
    "    [0.15406714, 0.61009504, 0.92627007, 0.31464061, 0.96504647],\n",
    "    [0.62869109, 0.96683923, 0.67307913, 0.76715599, 0.20064658],\n",
    "    [0.13946135, 0.60860335, 0.12874316, 0.01669904, 0.56292157],\n",
    "    [0.94717694, 0.95504354, 0.23629217, 0.89741572, 0.40860903],\n",
    "    [0.45281581, 0.20396239, 0.96098094, 0.93523158, 0.05332813],\n",
    "    [0.82914701, 0.54004522, 0.41405477, 0.42009503, 0.27472549],\n",
    "])\n",
    "\n",
    "theta = np.asarray([\n",
    "    [0.83951097, 0.40424355, 0.31119862, 0.71417276, 0.12546929],\n",
    "    [0.17836209, 0.44846427, 0.73829393, 0.48937756, 0.08848055],\n",
    "])\n",
    "\n",
    "temp_parameter = 0.39322872262244546\n",
    "\n",
    "solution = np.asarray([\n",
    "    [0.49780446, 0.59075737, 0.71055499, 0.57078338, 0.36703235, 0.66255384, 0.52216204, 0.85568887, 0.55831068, 0.759442],\n",
    "    [0.50219554, 0.40924263, 0.28944501, 0.42921662, 0.63296765, 0.33744616, 0.47783796, 0.14431113, 0.44168932, 0.240558],\n",
    "])\n",
    "\n",
    "H = compute_probabilities(X, theta, temp_parameter)\n",
    "\n",
    "print(\"Solution:\", solution.shape)\n",
    "print(\"H:\", H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function(X, Y, theta, lambda_factor, temp_parameter):\n",
    "    \"\"\"\n",
    "    Computes the total cost over every datapoint.\n",
    "\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns\n",
    "        c - the cost value (scalar)\n",
    "    \"\"\"\n",
    "    k = theta.shape[0]\n",
    "    n = X.shape[0]\n",
    "    clip_prob_matrix = np.clip(compute_probabilities(X, theta, temp_parameter), 1e-15, 1-1e-15)\n",
    "    log_clip_matrix = np.log(clip_prob_matrix)\n",
    "    M = sparse.coo_matrix(([1]*n, (Y, range(n))), shape = (k,n)).toarray()\n",
    "    error_term = (-1/n)*np.sum(log_clip_matrix[M==1])\n",
    "    reg_term = (lambda_factor/2)*np.linalg.norm(theta)**2\n",
    "    return error_term + reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_descent_iteration(X, Y, theta, alpha, lambda_factor, temp_parameter):\n",
    "    \"\"\"\n",
    "    Runs one step of batch gradient descent\n",
    "\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        alpha - the learning rate (scalar)\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns:\n",
    "        theta - (k, d) NumPy array that is the final value of parameters theta\n",
    "    \"\"\"\n",
    "    itemp = 1./temp_parameter\n",
    "    num_examples = X.shape[0]\n",
    "    num_labels = theta.shape[0]\n",
    "    probabilities = compute_probabilities(X, theta, temp_parameter)\n",
    "    M = sparse.coo_matrix(([1]*num_examples, (Y,range(num_examples))), shape=(num_labels,num_examples)).toarray()\n",
    "    non_regularized_gradient = np.dot(M-probabilities, X)\n",
    "    non_regularized_gradient *= -itemp/num_examples\n",
    "    return theta - alpha * (non_regularized_gradient + lambda_factor * theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 0.10050000000000003\n"
     ]
    }
   ],
   "source": [
    "from mnist.part1.softmax import softmax_regression, compute_test_error\n",
    "\n",
    "temp_parameter = 1\n",
    "\n",
    "theta, cost_function_history = softmax_regression(train_x, train_y, temp_parameter, alpha=0.3, lambda_factor=1.0e-4, k=10, num_iterations=150)\n",
    "test_error = compute_test_error(test_x, test_y, theta, temp_parameter)\n",
    "\n",
    "print(\"Test error:\", test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### 6. Changing Labels\n",
    "\n",
    "#### Using the Current Model - Update Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_y(train_y, test_y):\n",
    "    \"\"\"\n",
    "    Changes the old digit labels for the training and test set for the new (mod 3)\n",
    "    labels.\n",
    "\n",
    "    Args:\n",
    "        train_y - (n, ) NumPy array containing the labels (a number between 0-9)\n",
    "                 for each datapoint in the training set\n",
    "        test_y - (n, ) NumPy array containing the labels (a number between 0-9)\n",
    "                for each datapoint in the test set\n",
    "\n",
    "    Returns:\n",
    "        train_y_mod3 - (n, ) NumPy array containing the new labels (a number between 0-2)\n",
    "                     for each datapoint in the training set\n",
    "        test_y_mod3 - (n, ) NumPy array containing the new labels (a number between 0-2)\n",
    "                    for each datapoint in the test set\n",
    "    \"\"\"\n",
    "    train_y_mod3 = np.mod(train_y,3)\n",
    "    test_y_mod3 = np.mod(test_y,3)\n",
    "    return train_y_mod3, test_y_mod3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_error_mod3(X, Y, theta, temp_parameter):\n",
    "    \"\"\"\n",
    "    Returns the error of these new labels when the classifier predicts the digit. (mod 3)\n",
    "\n",
    "    Args:\n",
    "        X - (n, d - 1) NumPy array (n datapoints each with d - 1 features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-2) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns:\n",
    "        test_error - the error rate of the classifier (scalar)\n",
    "    \"\"\"\n",
    "    from mnist.part1.softmax import get_classification\n",
    "    predicted_label = get_classification(X,theta,temp_parameter)\n",
    "    test_error = 1 - np.mean(np.mod(predicted_label,3)==Y)\n",
    "    return test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('Machine_Learning_with_Python-7V858pgh')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbf55fd141a929e96d217693ff943f340da010ee827d3bb7b01df8ee1539f1d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
