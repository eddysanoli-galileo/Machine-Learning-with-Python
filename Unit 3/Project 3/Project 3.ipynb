{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3: Digit Recognition\n",
    "\n",
    "Good programmers can use neural nets. Great programmers can make them. This section will guide you through the implementation of a simple neural net with an architecture as shown in the figure below. You will implement the net from scratch (you will probably never do this again, don't worry) so that you later feel confident about using libraries. We provide some skeleton code in neural_nets.py for you to fill in.\n",
    "\n",
    "![neural_net](../Media/images_neuralnet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Activation Functions\n",
    "\n",
    "#### Rectified Linear Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectified_linear_unit(x):\n",
    "    \"\"\" Returns the ReLU of x, or the maximum between 0 and x.\"\"\"\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking the Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectified_linear_unit_derivative(x):\n",
    "    \"\"\" Returns the derivative of ReLU.\"\"\"\n",
    "\n",
    "    x[x > 0] = 1\n",
    "    x[x <= 0] = 0\n",
    "\n",
    "    # ReLu returns 1 for all positive values and 0 for all negative values\n",
    "    # (Returns 0 when the value is equal to 0 as well)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### 4. Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_layer_activation(x):\n",
    "    \"\"\" Linear function, returns input as is. \"\"\"\n",
    "    return x\n",
    "\n",
    "def output_layer_activation_derivative(x):\n",
    "    \"\"\" Returns the derivative of a linear function: 1. \"\"\"\n",
    "    return 1\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \"\"\"\n",
    "        Contains the following functions:\n",
    "            -train: tunes parameters of the neural network based on error obtained from forward propagation.\n",
    "            -predict: predicts the label of a feature vector based on the class's parameters.\n",
    "            -train_neural_network: trains a neural network over all the data points for the specified number of epochs during initialization of the class.\n",
    "            -test_neural_network: uses the parameters specified at the time in order to test that the neural network classifies the points given in testing_points within a margin of error.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # DO NOT CHANGE PARAMETERS (Initialized to floats instead of ints)\n",
    "        self.input_to_hidden_weights = np.matrix('1. 1.; 1. 1.; 1. 1.')  # (3,2)\n",
    "        self.hidden_to_output_weights = np.matrix('1. 1. 1.')\n",
    "        self.biases = np.matrix('0.; 0.; 0.')\n",
    "        self.learning_rate = .001\n",
    "        self.epochs_to_train = 10\n",
    "        self.training_points = [((2,1), 10), ((3,3), 21), ((4,5), 32), ((6, 6), 42)]\n",
    "        self.testing_points = [(1,1), (2,2), (3,3), (5,5), (10,10)]\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "\n",
    "    def train(self, x1 : float, x2: float, y):\n",
    "\n",
    "        ### Forward propagation ###\n",
    "        input_values = np.matrix([[x1],[x2]]) # 2 by 1\n",
    "\n",
    "        # Calculate the input and activation of the hidden layer\n",
    "        hidden_layer_weighted_input = np.dot(self.input_to_hidden_weights, input_values) + self.biases  # (3,2) * (2x1) = (3,1) + (3,1) = (3,1)\n",
    "        hidden_layer_activation = rectified_linear_unit(hidden_layer_weighted_input)                    # (3,1)\n",
    "\n",
    "        output = np.dot(self.hidden_to_output_weights, hidden_layer_activation)                         # (1,3) * (3,1) = (1,1)\n",
    "        activated_output = output_layer_activation(output)\n",
    "\n",
    "        ### Backpropagation ###\n",
    "\n",
    "        # Compute gradients\n",
    "        output_layer_error = (y - activated_output)                                                                                 # Derivative of cost function\n",
    "        hidden_layer_error = self.hidden_to_output_weights.T * output_layer_activation_derivative(output) *  output_layer_error     # (3 by 1 matrix)\n",
    "\n",
    "        bias_gradients = hidden_layer_error * 1                                                     # Derivative of Z with respect of the bias is 1 (Z = W*a + b) \n",
    "        hidden_to_output_weight_gradients = np.dot(output_layer_error, hidden_layer_activation.T)   # Derivative of Z with respect of the weights is the weighted input of the layer (Z' = a)\n",
    "        input_to_hidden_weight_gradients = np.dot(hidden_layer_error, input_values.T)\n",
    "        \n",
    "        # print(bias_gradients.shape)\n",
    "        # print(input_to_hidden_weight_gradients.shape)\n",
    "        # print(hidden_to_output_weight_gradients.shape)\n",
    "        # print(\"======================\")\n",
    "\n",
    "        # Use gradients to adjust weights and biases using gradient descent\n",
    "        self.biases = self.biases - self.learning_rate * bias_gradients\n",
    "        self.input_to_hidden_weights = self.input_to_hidden_weights - self.learning_rate * input_to_hidden_weight_gradients\n",
    "        self.hidden_to_output_weights = self.hidden_to_output_weights - self.learning_rate * hidden_to_output_weight_gradients\n",
    "\n",
    "    # ============================================================\n",
    "\n",
    "    def predict(self, x1, x2):\n",
    "\n",
    "        input_values = np.matrix([[x1],[x2]])\n",
    "        print(input_values.shape)\n",
    "        print(self.input_to_hidden_weights.shape)\n",
    "\n",
    "        # Compute output for a single input(should be same as the forward propagation in training)\n",
    "        hidden_layer_weighted_input = np.dot(self.input_to_hidden_weights, input_values) + self.biases\n",
    "        hidden_layer_activation = rectified_linear_unit(hidden_layer_weighted_input)\n",
    "        output = np.dot(self.hidden_to_output_weights, hidden_layer_activation)\n",
    "\n",
    "        activated_output = output_layer_activation(output)\n",
    "        return activated_output.item()\n",
    "\n",
    "    # Run this to train your neural network once you complete the train method\n",
    "    def train_neural_network(self):\n",
    "\n",
    "        for epoch in range(self.epochs_to_train):\n",
    "            for x,y in self.training_points:\n",
    "                self.train(x[0], x[1], y)\n",
    "\n",
    "    # Run this to test your neural network implementation for correctness after it is trained\n",
    "    def test_neural_network(self):\n",
    "\n",
    "        for point in self.testing_points:\n",
    "            print(\"Point,\", point, \"Prediction,\", self.predict(point[0], point[1]))\n",
    "            if abs(self.predict(point[0], point[1]) - 7*point[0]) < 0.1:\n",
    "                print(\"Test Passed\")\n",
    "            else:\n",
    "                print(\"Point \", point[0], point[1], \" failed to be predicted correctly.\")\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "(3, 2)\n",
      "Point, (1, 1) Prediction, -inf\n",
      "(2, 1)\n",
      "(3, 2)\n",
      "Point  1 1  failed to be predicted correctly.\n"
     ]
    }
   ],
   "source": [
    "x = NeuralNetwork()\n",
    "x.train_neural_network()\n",
    "x.test_neural_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Fully-Connected Neural Networks\n",
    "\n",
    "#### Training and Testing Accuracy Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle, gzip\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from .mnist.utils import *\n",
    "from .mnist.part2-mnist.train_utils import batchify_data, run_epoch, train_model\n",
    "\n",
    "# Specify seed for deterministic behavior, then shuffle. Do not change seed for official submissions to edx\n",
    "np.random.seed(12321)  # for reproducibility\n",
    "torch.manual_seed(12321)  # for reproducibility\n",
    "\n",
    "# Load the dataset\n",
    "num_classes = 10\n",
    "X_train, y_train, X_test, y_test = get_MNIST_data()\n",
    "\n",
    "# Split into train and dev\n",
    "dev_split_index = int(9 * len(X_train) / 10)\n",
    "X_dev = X_train[dev_split_index:]\n",
    "y_dev = y_train[dev_split_index:]\n",
    "X_train = X_train[:dev_split_index]\n",
    "y_train = y_train[:dev_split_index]\n",
    "\n",
    "permutation = np.array([i for i in range(len(X_train))])\n",
    "np.random.shuffle(permutation)\n",
    "X_train = [X_train[i] for i in permutation]\n",
    "y_train = [y_train[i] for i in permutation]\n",
    "\n",
    "# Split dataset into batches\n",
    "batch_size = 32\n",
    "train_batches = batchify_data(X_train, y_train, batch_size)\n",
    "dev_batches = batchify_data(X_dev, y_dev, batch_size)\n",
    "test_batches = batchify_data(X_test, y_test, batch_size)\n",
    "\n",
    "#################################\n",
    "## Model specification TODO\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(784, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10),\n",
    "        )\n",
    "lr=0.1\n",
    "momentum=0\n",
    "##################################\n",
    "\n",
    "train_model(train_batches, dev_batches, model, lr=lr, momentum=momentum)\n",
    "\n",
    "## Evaluate the model on test data\n",
    "loss, accuracy = run_epoch(test_batches, model.eval(), None)\n",
    "\n",
    "print (\"Loss on test set:\"  + str(loss) + \" Accuracy on test set: \" + str(accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('Machine_Learning_with_Python-7V858pgh')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbf55fd141a929e96d217693ff943f340da010ee827d3bb7b01df8ee1539f1d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
