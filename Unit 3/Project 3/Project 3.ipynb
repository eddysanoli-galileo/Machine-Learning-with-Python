{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3: Digit Recognition\n",
    "\n",
    "Good programmers can use neural nets. Great programmers can make them. This section will guide you through the implementation of a simple neural net with an architecture as shown in the figure below. You will implement the net from scratch (you will probably never do this again, don't worry) so that you later feel confident about using libraries. We provide some skeleton code in neural_nets.py for you to fill in.\n",
    "\n",
    "![neural_net](../Media/images_neuralnet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Activation Functions\n",
    "\n",
    "#### Rectified Linear Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectified_linear_unit(x):\n",
    "    \"\"\" Returns the ReLU of x, or the maximum between 0 and x.\"\"\"\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking the Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectified_linear_unit_derivative(x):\n",
    "    \"\"\" Returns the derivative of ReLU.\"\"\"\n",
    "\n",
    "    x[x > 0] = 1\n",
    "    x[x <= 0] = 0\n",
    "\n",
    "    # ReLu returns 1 for all positive values and 0 for all negative values\n",
    "    # (Returns 0 when the value is equal to 0 as well)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### 4. Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_layer_activation(x):\n",
    "    \"\"\" Linear function, returns input as is. \"\"\"\n",
    "    return x\n",
    "\n",
    "def output_layer_activation_derivative(x):\n",
    "    \"\"\" Returns the derivative of a linear function: 1. \"\"\"\n",
    "    return 1\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \"\"\"\n",
    "        Contains the following functions:\n",
    "            -train: tunes parameters of the neural network based on error obtained from forward propagation.\n",
    "            -predict: predicts the label of a feature vector based on the class's parameters.\n",
    "            -train_neural_network: trains a neural network over all the data points for the specified number of epochs during initialization of the class.\n",
    "            -test_neural_network: uses the parameters specified at the time in order to test that the neural network classifies the points given in testing_points within a margin of error.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # DO NOT CHANGE PARAMETERS (Initialized to floats instead of ints)\n",
    "        self.input_to_hidden_weights = np.matrix('1. 1.; 1. 1.; 1. 1.')  # (3,2)\n",
    "        self.hidden_to_output_weights = np.matrix('1. 1. 1.')\n",
    "        self.biases = np.matrix('0.; 0.; 0.')\n",
    "        self.learning_rate = .001\n",
    "        self.epochs_to_train = 10\n",
    "        self.training_points = [((2,1), 10), ((3,3), 21), ((4,5), 32), ((6, 6), 42)]\n",
    "        self.testing_points = [(1,1), (2,2), (3,3), (5,5), (10,10)]\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, x1 : float, x2: float, y):\n",
    "\n",
    "        ### Forward propagation ###\n",
    "        input_values = np.matrix([[x1],[x2]]) # 2 by 1\n",
    "\n",
    "        # Calculate the input and activation of the hidden layer\n",
    "        hidden_layer_weighted_input = np.dot(self.input_to_hidden_weights, input_values)        # (3,2) * (2x1) = (3,1)\n",
    "        hidden_layer_activation = rectified_linear_unit(hidden_layer_weighted_input)            # (3,1)\n",
    "\n",
    "        output = np.dot(self.hidden_to_output_weights, hidden_layer_activation) + self.biases  # (1,3) * (3,1) = (1,1)\n",
    "        activated_output = output_layer_activation(output)\n",
    "\n",
    "        ### Backpropagation ###\n",
    "\n",
    "        # Compute gradients\n",
    "        output_layer_error = (y - activated_output)                                                 # Derivative of cost function                                                        # Scalar (Also called C)\n",
    "        hidden_layer_error = output_layer_activation_derivative(output) *  output_layer_error       # (3 by 1 matrix)\n",
    "\n",
    "        bias_gradients = hidden_layer_error * 1                                                     # Derivative of Z with respect of the bias is 1 (Z = W*a + b) \n",
    "        hidden_to_output_weight_gradients = np.dot(hidden_layer_error, hidden_layer_activation.T)   # Derivative of Z with respect of the weights is the weighted input of the layer (Z' = a)\n",
    "        \n",
    "        dy_du = hidden_layer_error\n",
    "        du_da = self.hidden_to_output_weights\n",
    "        da_dz = rectified_linear_unit_derivative(hidden_layer_weighted_input)\n",
    "        dz_dw = input_values\n",
    "\n",
    "        # Going step by step backwards\n",
    "        print(\"U Level:\", du_da.T.shape, \"*\", dy_du.shape)\n",
    "        u_level = du_da @ dy_du\n",
    "        print(\"=\", u_level.shape)\n",
    "        print(\"A Level:\", da_dz.shape, \"*\", u_level.shape)\n",
    "        a_level = da_dz @ u_level\n",
    "        print(\"=\", a_level.shape)\n",
    "        print(\"Z Level:\", dz_dw.shape, \"*\", a_level.shape)\n",
    "        z_level = a_level @ dz_dw.T\n",
    "        print(\"=\", z_level.shape)\n",
    "        input_to_hidden_weight_gradients = z_level.copy()\n",
    "        print(\"======================\")\n",
    "        \n",
    "        # Use gradients to adjust weights and biases using gradient descent\n",
    "        self.biases = self.biases - self.learning_rate * bias_gradients\n",
    "        self.input_to_hidden_weights = self.input_to_hidden_weights - self.learning_rate * input_to_hidden_weight_gradients\n",
    "        self.hidden_to_output_weights = self.hidden_to_output_weights - self.learning_rate * hidden_to_output_weight_gradients\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, x1, x2):\n",
    "\n",
    "        input_values = np.matrix([[x1],[x2]])\n",
    "\n",
    "        # Compute output for a single input(should be same as the forward propagation in training)\n",
    "        hidden_layer_weighted_input = np.dot(self.input_to_hidden_weights, input_values)\n",
    "        hidden_layer_activation = rectified_linear_unit(hidden_layer_weighted_input)\n",
    "        output = np.dot(self.hidden_to_output_weights, hidden_layer_activation) + self.biases\n",
    "\n",
    "        activated_output = output_layer_activation(output)\n",
    "\n",
    "        return activated_output.item()\n",
    "\n",
    "    # Run this to train your neural network once you complete the train method\n",
    "    def train_neural_network(self):\n",
    "\n",
    "        for epoch in range(self.epochs_to_train):\n",
    "            for x,y in self.training_points:\n",
    "                self.train(x[0], x[1], y)\n",
    "\n",
    "    # Run this to test your neural network implementation for correctness after it is trained\n",
    "    def test_neural_network(self):\n",
    "\n",
    "        for point in self.testing_points:\n",
    "            print(\"Point,\", point, \"Prediction,\", self.predict(point[0], point[1]))\n",
    "            if abs(self.predict(point[0], point[1]) - 7*point[0]) < 0.1:\n",
    "                print(\"Test Passed\")\n",
    "            else:\n",
    "                print(\"Point \", point[0], point[1], \" failed to be predicted correctly.\")\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [[3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]]\n",
      "Train: [[18.74182256 18.74182256 18.74182256]\n",
      " [18.74182256 18.74182256 18.74182256]\n",
      " [18.74182256 18.74182256 18.74182256]]\n",
      "Train: [[56.22961604 56.22961604 56.22961604]\n",
      " [56.22961604 56.22961604 56.22961604]\n",
      " [56.22961604 56.22961604 56.22961604]]\n",
      "Train: [[134.98720902 134.98720902 134.98720902]\n",
      " [134.98720902 134.98720902 134.98720902]\n",
      " [134.98720902 134.98720902 134.98720902]]\n",
      "Train: [[10.69980348 10.69980348 10.69980348]\n",
      " [10.69980348 10.69980348 10.69980348]\n",
      " [10.69980348 10.69980348 10.69980348]]\n",
      "Train: [[46.8125754 46.8125754 46.8125754]\n",
      " [46.8125754 46.8125754 46.8125754]\n",
      " [46.8125754 46.8125754 46.8125754]]\n",
      "Train: [[106.91963496 106.91963496 106.91963496]\n",
      " [106.91963496 106.91963496 106.91963496]\n",
      " [106.91963496 106.91963496 106.91963496]]\n",
      "Train: [[147.84090212 147.84090212 147.84090212]\n",
      " [147.84090212 147.84090212 147.84090212]\n",
      " [147.84090212 147.84090212 147.84090212]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = NeuralNetwork()\n",
    "x.train_neural_network()\n",
    "# x.test_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('Machine_Learning_with_Python-7V858pgh')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbf55fd141a929e96d217693ff943f340da010ee827d3bb7b01df8ee1539f1d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
