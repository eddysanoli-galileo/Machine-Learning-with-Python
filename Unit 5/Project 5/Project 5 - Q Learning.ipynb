{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 5\n",
    "\n",
    "In this project, we address the task of learning control policies for text-based games using reinforcement learning. In these games, all interactions between players and the virtual world are through text. The current world state is described by elaborate text, and the underlying state is not directly observable. Players read descriptions of the state and respond with natural language commands to take actions.\n",
    "\n",
    "For this project you will conduct experiments on a small Home World, which mimic the environment of a typical house.The world consists of a few rooms, and each room contains a representative object that the player can interact with. For instance, the kitchen has an apple that the player can eat. The goal of the player is to finish some quest. An example of a quest given to the player in text is You are hungry now . To complete this quest, the player has to navigate through the house to reach the kitchen and eat the apple. In this game, the room is hidden from the player, who only receives a description of the underlying room. At each step, the player read the text describing the current room and the quest, and respond with some command (e.g., eat apple ). The player then receives some reward that depends on the state and his/her command.\n",
    "\n",
    "In order to design an autonomous game player, we will employ a reinforcement learning framework to learn command policies using game rewards as feedback. Since the state observable to the player is described in text, we have to choose a mechanism that maps text descriptions into vector representations. A naive approach is to create a map that assigns a unique index for each text description. However, such approach becomes difficult to implement when the number of textual state descriptions are huge. An alternative method is to use a bag-of-words representation derived from the text description. This project requires you to complete the following tasks:\n",
    "\n",
    "1. Implement the tabular Q-learning algorithm for a simple setting where each text description is associated with a unique index.\n",
    "2. Implement the Q-learning algorithm with linear approximation architecture, using bag-of-words representation for textual state description.\n",
    "3. Implement a deep Q-network.\n",
    "4. Use your Q-learning algorithms on the Home World game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "\n",
    "### 2. Home World Game\n",
    "\n",
    "#### Optimal Episodic Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Optimal Reward (E[V*]): 0.55375\n"
     ]
    }
   ],
   "source": [
    "# Number of rooms\n",
    "rooms = 4\n",
    "\n",
    "# Quests\n",
    "quests = 4\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.5\n",
    "\n",
    "# Total number of \"room-quest\" pairs\n",
    "pairs = rooms * quests\n",
    "\n",
    "\n",
    "# Lets analyze one case: Starting in the living room\n",
    "# - Living - Q:Living   = Correct command in 1 step = +1\n",
    "# - Living - Q:Garden   = 1 step (No discount) + action (Reward * gamma) = -0.01 + 1*0.5\n",
    "# - Living - Q:Bedroom  = 1 step (No discount) + action (Reward * gamma) = -0.01 + 1*0.5\n",
    "# - Living - Q:Kitchen  = 2 step (1st - No discount / 2nd - discounted) + action (R * gamma) = -0.01 - 0.01*0.5 + 1*0.5^2\n",
    "\n",
    "# Since we want the \"optimal\" reward, we assume that the agent behaves perfectly with\n",
    "# no errors. This means that it only moves the exact number of steps required to get to\n",
    "# its goal and then immediately executes the correct action. Assuming this, we have three \n",
    "# cases: One that requires no steps, one that requires 1 step and one that requires 2 steps\n",
    "# to reach the quest's goal. This repeats a total of 4 times since the map is a square: No\n",
    "# mater the starting room, you will always require 0 steps to get to the goal if its in the\n",
    "# same room, 1 step to get to the two adjacent rooms and 2 steps to get to the room found in\n",
    "# the opposite room of the map.\n",
    "\n",
    "# ===========================\n",
    "# OPTIMAL VALUE\n",
    "\n",
    "# NOTES:\n",
    "# 1. The probability of each \"room-quest\" occurring is 1/16 because there are 16\n",
    "#    different \"pairs\" and each one has an equal probability of occurring.\n",
    "# 2. Remember that the expected value is equal to the quantity that we are trying to\n",
    "#    get the expected value from (In this case the optimal Value V*), weighted by the\n",
    "#    probability of said event ocurring in the first place. We just sum all of the \n",
    "#    weighted terms for each \"room-quest\" pair and we have the total value V*\n",
    "\n",
    "# Zero steps are required to get to the goal\n",
    "# V*(0 Steps) = 1/16 * 1\n",
    "EV_star_0 = 1/pairs * 1\n",
    "\n",
    "# One step required to get to the goal\n",
    "# V*(1 Steps) = 1/24 * (-0.01 + 1*0.5) * 2 (There are 2 cases where 1 step is needed)\n",
    "EV_star_1 = 2/pairs * (-0.01 + 1 * gamma)\n",
    "\n",
    "# Two steps required to get to the goal\n",
    "# V*(2 Steps) = 1/24 * (-0.01 - 0.01*0.5 + 1*0.5^2)\n",
    "EV_star_2 = 1/16 * (-0.01 - 0.01*gamma + 1 * gamma**2)\n",
    "\n",
    "# We multiply by 4 because we have 4 starting rooms\n",
    "EV_star = 4* ( EV_star_0 + EV_star_1 + EV_star_2 )\n",
    "print(\"Expected Optimal Reward (E[V*]):\", EV_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "### 3. Q-Learning Algorithm\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "Imports and constants used in the following sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "# Constants\n",
    "GAMMA = 0.5             # discounted factor\n",
    "ALPHA = 0.1             # learning rate for training\n",
    "\n",
    "# Epsilon\n",
    "TRAINING_EP = 1         # epsilon-greedy parameter for training\n",
    "TESTING_EP = 0.05       # epsilon-greedy parameter for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Step Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_q_learning(\n",
    "    q_func : np.ndarray, \n",
    "    current_state_1 : int, current_state_2 : int, \n",
    "    action_index : int,\n",
    "    object_index : int, \n",
    "    reward : float, \n",
    "    next_state_1 : int, next_state_2 : int,\n",
    "    terminal : bool\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Update q_func for a given transition\n",
    "\n",
    "    Args:\n",
    "        q_func (np.ndarray): current Q-function\n",
    "        current_state_1 (int): index of the current room (S_r)\n",
    "        current_state_2 (int): index of the current quest (S_q)\n",
    "        action_index (int): index of the current action\n",
    "        object_index (int): index of the current object\n",
    "        reward (float): the immediate reward the agent receives from playing current command\n",
    "        next_state_1 (int): index describing the next room (S'_r) \n",
    "        next_state_2 (int,): index describing the next quest (S'_q)\n",
    "        terminal (bool): True if this episode is over\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    ALPHA = 1           # Exponential running average constant\n",
    "    GAMMA = 0.5         # Discount factor\n",
    "\n",
    "    # Shorter names for states and actions\n",
    "    sr : int = current_state_1\n",
    "    sq : int = current_state_2\n",
    "    a : int = action_index\n",
    "    b : int = object_index\n",
    "    sr_prime : int = next_state_1\n",
    "    sq_prime : int = next_state_2\n",
    "\n",
    "    # This is the last step in the episode there are no \"future events\"\n",
    "    if terminal:\n",
    "\n",
    "        # We set the max value for all future actions to zero, since there are no more\n",
    "        # actions after this\n",
    "        max_Q_next = 0\n",
    "    \n",
    "    else: \n",
    "\n",
    "        # Get the maximum Q value that we can get from taking one of the C' possible\n",
    "        # actions (a, b)\n",
    "        max_Q_next = np.max(q_func[sr_prime, sq_prime, :, :])\n",
    "    \n",
    "    # Update according to the Q-learning algorithm\n",
    "    # - q_func[sr, sq, a, b] : scalar\n",
    "    # - reward : scalar\n",
    "    # - max_Q : scalar\n",
    "    q_update = (1 - ALPHA) * q_func[sr, sq, a, b] + (ALPHA) * (reward + GAMMA * max_Q_next)\n",
    "\n",
    "    # Q(s, c) = updated Q(s, c)\n",
    "    # - s = (sr, sq)\n",
    "    #   sr: (current_state_1) Current room\n",
    "    #   sq: (current_state_2) Current quest\n",
    "    # - c = (a, b)\n",
    "    #   a: (action_index) Action to execute\n",
    "    #   b: (object_index) Object that will \"receive\" the action to execute (ie. eat APPLE)\n",
    "    q_func[current_state_1, current_state_2, action_index, object_index] = q_update\n",
    "\n",
    "    # This function shouldn't return anything\n",
    "    return None  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-greedy Exploration\n",
    "\n",
    "Note that the Q-learning algorithm does not specify how we should interact in the world so as to learn quickly. It merely updates the values based on the experience collected. If we explore randomly, i.e., always select actions at random, we would most likely not get anywhere. A better option is to exploit what we have already learned, as summarized by current Q-values. We can always act greedily with respect to the current estimates, i.e., take an action $\\pi (s)=\\arg \\max _{c\\in C}Q(s,c)$. Of course, early on, these are not necessarily very good actions. For this reason, a typical exploration strategy is to follow a so-called -greedy policy: with probability $\\epsilon$ take a random action out of $C$ with probability $1-\\epsilon$ follow $\\pi (s)=\\arg \\max _{c\\in C}Q(s,c)$. The value of $\\epsilon$ here balances exploration vs exploitation. A large value of  means exploring more (randomly), not using much of what we have learned. A small $\\epsilon$, on the other hand, will generate experience consistent with the current estimates of Q-values.\n",
    "\n",
    "Now you will write a function `epsilon_greedy` that implements the -greedy exploration policy using the current Q-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(\n",
    "    state_1 : int,\n",
    "    state_2 : int,\n",
    "    q_func : np.ndarray, \n",
    "    epsilon : float\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Returns an action selected by an epsilon-Greedy exploration policy\n",
    "\n",
    "    Args:\n",
    "        state_1 (int): Index describing the current room (Sr)\n",
    "        state_2 (int): Index describing the current quest (Sq)\n",
    "        q_func (np.ndarray): current Q-function (4D array)\n",
    "        epsilon (float): the probability of choosing a random command\n",
    "\n",
    "    Returns:\n",
    "        (int, int): the indices describing the action/object to take (a, b) where \n",
    "        \"a\" is the action and \"b\" is the object on which to act upon.\n",
    "    \"\"\"\n",
    "\n",
    "    # Variable aliases\n",
    "    sr : int = state_1\n",
    "    sq : int = state_2\n",
    "\n",
    "    # We generate a random number between 0 and 1. If that number is smaller than (1- epsilon)\n",
    "    # we exploit. In any other case, we explore.\n",
    "    random_number : float = np.random.random_sample()\n",
    "    greedy : bool = (random_number < (1-epsilon))\n",
    "\n",
    "    # Exploit (Greedy action):\n",
    "    # Take the best possible action according to the current policy Pi(s)\n",
    "    if greedy:\n",
    "\n",
    "        # All possible actions for a given state S (2D array)\n",
    "        q_s : np.ndarray = q_func[sr, sq, :, :]\n",
    "\n",
    "        # Greedy action (Action C (a,b) that maximizes the Q value)\n",
    "        # NOTE: \"argmax\" flattens the 2D array and returns the linear index that corresponds\n",
    "        # to the highest value in that 2D array. We use \"unravel_index\" to convert that linear\n",
    "        # index back to a 2D index\n",
    "        pi_s = np.unravel_index(np.argmax(q_s), q_s.shape)\n",
    "\n",
    "        # We extract the action and object from the policy for the current state (Pi(s))\n",
    "        a, b = pi_s\n",
    "\n",
    "    # Explore (Non-greedy action):\n",
    "    # Take a random action from all the possible ones\n",
    "    else:\n",
    "\n",
    "        # Random action\n",
    "        _, _, NUM_ACTIONS, NUM_OBJECTS = q_func.shape\n",
    "\n",
    "        # Select a random action and object\n",
    "        a : int = np.random.randint(NUM_ACTIONS)\n",
    "        b : int = np.random.randint(NUM_OBJECTS)\n",
    "     \n",
    "    # Return a tuple \"C\" of an action and an object\n",
    "    return (a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### 4. Tabular Q-Learning for Home World Game\n",
    "\n",
    "In this section you will evaluate the tabular Q-learning algorithms for the Home world game. Recall that the state observable to the player is described in text. Therefore we have to choose a mechanism that maps text descriptions into vector representations.\n",
    "\n",
    "In this section you will consider a simple approach that assigns a unique index for each text description. In particular, we will build two dictionaries:\n",
    "\n",
    "- `dict_room_desc` that takes the room description text as the key and returns a unique scalar index\n",
    "- `dict_quest_desc` that takes the quest description text as the key and returns a unique scalar index.\n",
    "\n",
    "For instance, consider an observable state $s=(s_{r},s_{q})$, where $s_r$ and $s_q$ are the text descriptions for the current room and the current request, respectively. Then $i_{r}=$ dict_room_desc $[s_r]$ gives the scalar index for $s_r$ and $i_{q}=$ dict_quest_desc $[s_q]$ gives the scalar index for $s_q$. That is, the textual state $s = (s_r, s_q)$ is mapped to a tuple $I = (i_r, i_q)$.\n",
    "\n",
    "Normally, we would build these dictionaries as we train our agent, collecting descriptions and adding them to the list of known descriptions. For the purpose of this project, these dictionaries will be provided to you.\n",
    "\n",
    "#### Evaluating Tabular Q-learning on Home World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.framework import make_all_states_index\n",
    "from rl import framework\n",
    "\n",
    "# Initialize the Q function values\n",
    "q_func : np.ndarray = np.zeros((4, 4, 3, 7))\n",
    "\n",
    "# Create the dictionaries that map text states to indexes\n",
    "dict_room_desc, dict_quest_desc = make_all_states_index()\n",
    "\n",
    "# Run a single episode\n",
    "# NOTE: In this example we have multiple experiments or runs:\n",
    "#   1. Run: Each run consists of multiple epochs\n",
    "#   2. Epoch: In each epoch we train and test the agent for X episodes\n",
    "#   3. Episode: Each episode consists of basically one go through the game\n",
    "def run_episode(for_training):\n",
    "    \"\"\" \n",
    "    Runs one episode\n",
    "    If for training, update Q function\n",
    "    If for testing, computes and return cumulative discounted reward\n",
    "\n",
    "    Args:\n",
    "        for_training (bool): True if for training\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Use a different epsilon depending on whether the current run is for training or testing\n",
    "    epsilon = TRAINING_EP if for_training else TESTING_EP\n",
    "\n",
    "    # The reward for the current episode starts at 0\n",
    "    episode_reward = 0\n",
    "\n",
    "    # Discount\n",
    "    # NOTE: Remember that in each episode the power of gamma increases by one.\n",
    "    # This means that it starts with a power of zero (gamma^0 = 1) and then in each\n",
    "    # step it increases by one (gamma^1 = gamma, gamma^2 = gamma^2, ...)\n",
    "    episode_gamma = 1\n",
    "\n",
    "    # Initialize a new game, we get back:\n",
    "    # - Description of the initial room\n",
    "    # - A description of the quest for this episode\n",
    "    # - A variable indicating if the game is done (Initially false to indicate that its starting)\n",
    "    (current_room_desc, current_quest_desc, terminal) = framework.newGame()\n",
    "\n",
    "\n",
    "    while not terminal:\n",
    "\n",
    "        # Map from a description (Sr, Sq) to an index (ir, iq) like the ones used previously\n",
    "        ir : int = dict_room_desc[current_room_desc]\n",
    "        iq : int = dict_quest_desc[current_quest_desc]\n",
    "\n",
    "        # Choose an action using the epsilon-greedy method\n",
    "        action_index, object_index = epsilon_greedy(ir, iq, q_func, epsilon)\n",
    "\n",
    "        # Aliases for the action and object indexes\n",
    "        a = action_index\n",
    "        b = object_index\n",
    "\n",
    "        # Run a step of the game. This returns:\n",
    "        # - Future room description (S'r)\n",
    "        # - Future quest description (S'q)\n",
    "        # - Reward (R)\n",
    "        # - Terminal: Whether the game has finished or not\n",
    "        next_room_desc, next_quest_desc, reward, terminal = framework.step_game(current_room_desc, current_quest_desc, a, b)\n",
    "\n",
    "        # Map from next state descriptions to indices\n",
    "        ir_prime : int = dict_room_desc[next_room_desc]\n",
    "        iq_prime : int = dict_quest_desc[next_quest_desc]\n",
    "\n",
    "        # Aliases for states\n",
    "        sr = ir\n",
    "        sq = iq\n",
    "        sr_prime = ir_prime\n",
    "        sq_prime = iq_prime\n",
    "\n",
    "        if for_training:\n",
    "\n",
    "            # Update the Q function\n",
    "            # NOTE: The update gets done \"in place\" meaning that we dont have to return anything\n",
    "            # the Q function get updated inside the function\n",
    "            tabular_q_learning(q_func, sr, sq, a, b, reward, sr_prime, sq_prime, terminal)\n",
    "\n",
    "        if not for_training:\n",
    "            \n",
    "            # Calculate the discounted reward\n",
    "            episode_reward += reward * episode_gamma\n",
    "\n",
    "            # Increase the power of the gamma used for next episode\n",
    "            episode_gamma *= GAMMA\n",
    "\n",
    "        # The current \"next_step\" will consist of the \"current_step\" in the\n",
    "        # next pass of the while loop\n",
    "        current_room_desc = next_room_desc\n",
    "        current_quest_desc = next_quest_desc\n",
    "\n",
    "    if not for_training:\n",
    "        return episode_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report Performance\n",
    "\n",
    "Function to reload the contents of `agent_tabular_ql.py` and `framework.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rl.framework' from 'e:\\\\Archivos\\\\Educación\\\\Posgrado\\\\Data Science (Universidad Galileo)\\\\2022 (MITx)\\\\Machine Learning with Python\\\\Unit 5\\\\Project 5\\\\rl\\\\framework.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl import agent_tabular_ql\n",
    "\n",
    "import importlib\n",
    "importlib.reload(agent_tabular_ql)\n",
    "importlib.reload(framework)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg reward: 0.497135 | Ewma reward: 0.522089: 100%|█| 200/200 [00:01<00:00, 110.\n",
      "Avg reward: 0.496757 | Ewma reward: 0.514315: 100%|█| 200/200 [00:01<00:00, 110.\n",
      "Avg reward: 0.500094 | Ewma reward: 0.513341: 100%|█| 200/200 [00:01<00:00, 112.\n",
      "Avg reward: 0.500189 | Ewma reward: 0.502134: 100%|█| 200/200 [00:01<00:00, 110.\n",
      "Avg reward: 0.480652 | Ewma reward: 0.490288: 100%|█| 200/200 [00:01<00:00, 109.\n",
      "Avg reward: 0.504686 | Ewma reward: 0.529358: 100%|█| 200/200 [00:01<00:00, 109.\n",
      "Avg reward: 0.504442 | Ewma reward: 0.527282: 100%|█| 200/200 [00:01<00:00, 112.\n",
      "Avg reward: 0.486569 | Ewma reward: 0.515312: 100%|█| 200/200 [00:01<00:00, 108.\n",
      "Avg reward: 0.502761 | Ewma reward: 0.504529: 100%|█| 200/200 [00:01<00:00, 111.\n",
      "Avg reward: 0.501385 | Ewma reward: 0.518346: 100%|█| 200/200 [00:01<00:00, 110.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rl.agent_tabular_ql import run\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================== #\n",
    "# PARAMETERS                                     #\n",
    "# ============================================== #\n",
    "\n",
    "# Runs, epochs and episodes\n",
    "NUM_RUNS = 10           # Number of train-test sequences we will use to average performance\n",
    "NUM_EPOCHS = 200        # Number of times we will train and test our agent\n",
    "NUM_EPIS_TRAIN = 25     # number of episodes for training at each epoch\n",
    "NUM_EPIS_TEST = 50      # number of episodes for testing\n",
    "\n",
    "# ============================================== #\n",
    "# RUN ALGORITHM                                  #\n",
    "# ============================================== #\n",
    "\n",
    "# Convert all the possible descriptions for a quest and room into\n",
    "# dictionaries that map the text with an index\n",
    "(dict_room_desc, dict_quest_desc) = framework.make_all_states_index()\n",
    "\n",
    "# Based on the dictionary created get the number or room descriptions and quests\n",
    "NUM_ROOM_DESC = len(dict_room_desc)\n",
    "NUM_QUESTS = len(dict_quest_desc)\n",
    "\n",
    "# Set up the game\n",
    "framework.load_game_data()\n",
    "\n",
    "# All the rewards retrieved from all the tests done in each run, for every epoch\n",
    "# shape NUM_RUNS * NUM_EPOCHS\n",
    "epoch_rewards_test = []  \n",
    "\n",
    "# Run the game and add a new \"total cumulative reward\"\n",
    "for _ in range(NUM_RUNS):\n",
    "    epoch_rewards_test.append(run(NUM_ROOM_DESC, NUM_QUESTS, dict_room_desc, dict_quest_desc))\n",
    "\n",
    "# Turn the list of rewards into an array\n",
    "epoch_rewards_test = np.array(epoch_rewards_test)\n",
    "\n",
    "# ============================================== #\n",
    "# PLOT                                           #\n",
    "# ============================================== #\n",
    "\n",
    "# X-axis: One tick per epoch\n",
    "x = np.arange(NUM_EPOCHS)\n",
    "\n",
    "# New plot\n",
    "fig, axis = plt.subplots()\n",
    "\n",
    "# Plot reward per epoch averaged per run\n",
    "axis.plot(x, np.mean(epoch_rewards_test, axis=0))  \n",
    "axis.set_xlabel('Epochs')\n",
    "axis.set_ylabel('reward')\n",
    "axis.set_title(f'Tablular Q-Learning')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "### 8. Linear Q-Learning\n",
    "\n",
    "#### Epsilon-greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from rl.agent_linear import tuple2index, index2tuple\n",
    "\n",
    "NUM_ACTIONS, NUM_OBJECTS = 1, 1\n",
    "\n",
    "def epsilon_greedy(\n",
    "    state_vector: np.ndarray, \n",
    "    theta: np.ndarray, \n",
    "    epsilon: float\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Returns an action selected by an epsilon-greedy exploration policy\n",
    "\n",
    "    Args:\n",
    "        state_vector (np.ndarray): Vector representation of the state Psi_r(s)\n",
    "        theta (np.ndarray): Matrix weights that help compute the weighted sum of\n",
    "        the \"state_vector\"\n",
    "        epsilon (float): The probability of choosing a random command\n",
    "\n",
    "    Returns:\n",
    "        (int, int): Indices describing the action/object (a, b) to take\n",
    "    \"\"\"\n",
    "    \n",
    "    # We generate a random number between 0 and 1. If that number is smaller than (1- epsilon)\n",
    "    # we exploit. In any other case, we explore.\n",
    "    random_number : float = np.random.random_sample()\n",
    "    greedy : bool = (random_number < (1-epsilon))\n",
    "    \n",
    "    # Exploit (Greedy action):\n",
    "    # Take the best possible action according to the current policy Pi(s)\n",
    "    if greedy:\n",
    "\n",
    "        # We get the values for the Q function approximation\n",
    "        # Shape: (C, S) x (S, 1) = (S, 1)\n",
    "        #   S: Number of states in new Psi(s) representation\n",
    "        #   C: Number of available actions (a * b = actions * objects)\n",
    "        q_value : np.ndarray = (theta @ state_vector)\n",
    "\n",
    "        # Greedy action (Action C (a,b) that maximizes the Q value)\n",
    "        # NOTE: \"argmax\" flattens the 2D array and returns the linear index that corresponds\n",
    "        # to the highest value in that 2D array. However, since the \"q_value\" is basically a\n",
    "        # column vector, this flattening does not matter\n",
    "        pi_s = int(np.argmax(q_value))\n",
    "\n",
    "        # We need to convert the index of the column array that contains all\n",
    "        # possible permutations of \"a\" and \"b\" (C), into a tuple of indexes that\n",
    "        # will index a new square matrix of size (a, b). This will help us extract\n",
    "        # the index of \"a\" and \"b\", getting the action and object in the process\n",
    "        a, b = index2tuple(pi_s)\n",
    "\n",
    "    # Explore (Non-greedy action):\n",
    "    # Take a random action from all the possible ones\n",
    "    else:\n",
    "\n",
    "        # Select a random action and object\n",
    "        a : int = np.random.randint(NUM_ACTIONS)\n",
    "        b : int = np.random.randint(NUM_OBJECTS)\n",
    "     \n",
    "    # Return a tuple \"C\" composed of both an action and an object\n",
    "    return (a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Q-Learning\n",
    "\n",
    "Write a function `linear_q_learning` that updates the theta weight matrix, given the transition data $(s,a,R(s,a),s')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ALPHA = 1           # Exponential running average constant\n",
    "GAMMA = 0.5         # Discount factor\n",
    "\n",
    "def linear_q_learning(\n",
    "    theta : np.ndarray, \n",
    "    current_state_vector : np.ndarray, \n",
    "    action_index : int, \n",
    "    object_index : int,\n",
    "    reward : float, \n",
    "    next_state_vector : np.ndarray, \n",
    "    terminal : bool\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Update theta for a given transition\n",
    "\n",
    "    Args:\n",
    "        theta (np.ndarray): current weight matrix\n",
    "        current_state_vector (np.ndarray): vector representation of current state\n",
    "        action_index (int): index of the current action\n",
    "        object_index (int): index of the current object\n",
    "        reward (float): the immediate reward the agent receives from playing current command\n",
    "        next_state_vector (np.ndarray): vector representation of next state\n",
    "        terminal (bool): True if this episode is over\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # This is the last step in the episode there are no \"future events\"\n",
    "    if terminal:\n",
    "\n",
    "        # We set the max value for all future actions to zero, since there are no more\n",
    "        # actions after this\n",
    "        max_Q_next = 0\n",
    "    \n",
    "    else:\n",
    "\n",
    "        # We build the Q function by using the next state vector\n",
    "        q_values_next : np.ndarray = theta @ next_state_vector\n",
    "\n",
    "        # Get the maximum Q value that we can get from taking one of the C' possible\n",
    "        # actions (a, b)\n",
    "        max_Q_next : float = float(np.max(q_values_next))\n",
    "\n",
    "\n",
    "    # Current Q-function\n",
    "    q_values_current : np.ndarray = theta @ current_state_vector\n",
    "\n",
    "    # Get the linear index that corresponds to the current action (a, b)\n",
    "    c = tuple2index(action_index, object_index)\n",
    "\n",
    "    # Select the Q function values that correspond to the current action C (a,b)\n",
    "    q_value_c : float = q_values_current[c]\n",
    "    \n",
    "    # NOTE:\n",
    "    # Now that the Q values are generated using a vector representation\n",
    "    # of state in conjunction with a value matrix, we dont actually need to\n",
    "    # update the Q values directly, but the parameters theta. For this we simply\n",
    "    # minimize a simple quadratic loss function:\n",
    "    # \n",
    "    #   L(theta)  = 0.5 * (y - Q(s, c theta))^2\n",
    "    #   L'(theta) = (y - Q(s, c, theta)) * phi(s,c)\n",
    "    # \n",
    "    # By taking its gradient, we can use said gradient to update the theta parameter\n",
    "    # by using a simple gradient descent update\n",
    "    # \n",
    "    #  Theta = Theta - alpha * (gradient)\n",
    "\n",
    "    # Target value \"y\" or the sampled version of the bellman operator\n",
    "    y : float = reward + (GAMMA * max_Q_next)\n",
    "\n",
    "    # Calculate the gradient of the loss function\n",
    "    loss_gradient = (y - q_value_c) * current_state_vector\n",
    "\n",
    "    # Theta parameter update using gradient descent\n",
    "    theta[c] = theta[c] + ALPHA * loss_gradient\n",
    "\n",
    "    # This function shouldn't return anything\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate linear Q-learning on Home World game\n",
    "\n",
    "Adapt your `run_episode` function to call `linear_Q_learning` and evaluate your performance using hyperparmeters:\n",
    "\n",
    "Set `NUM_RUNS = 5`, `NUM_EPIS_TRAIN = 25`, `NUM_EPIS_TEST = 50`, $\\gamma = 0.5$, `TRAINING_EP = 0.5`, `TESTING_EP = 0.05` and the learning rate $\\alpha = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl import utils\n",
    "\n",
    "# Get a dictionary that allows us to translate text to a state vector using\n",
    "# the bag of words method (Basically we get a giant sparse vector with one position for\n",
    "# each unique word. The vector for a word is just giant column of zeros with a 1 where\n",
    "# the index for that specific word is found)\n",
    "state_texts = utils.load_data('./rl/game.tsv')\n",
    "dictionary = utils.bag_of_words(state_texts)\n",
    "\n",
    "# Theta is defined outside of the run episode function\n",
    "theta : np.ndarray\n",
    "\n",
    "def run_episode(for_training):\n",
    "    \"\"\" \n",
    "    Runs one episode\n",
    "    If for training, update Q function\n",
    "    If for testing, computes and return cumulative discounted reward\n",
    "\n",
    "    Args:\n",
    "        for_training (bool): True if for training\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Use a different epsilon depending on whether the current run is for training or testing\n",
    "    epsilon = TRAINING_EP if for_training else TESTING_EP\n",
    "    \n",
    "    # The reward for the current episode starts at 0\n",
    "    episode_reward : float = 0.0\n",
    "\n",
    "    # Discount\n",
    "    # NOTE: Remember that in each episode the power of gamma increases by one.\n",
    "    # This means that it starts with a power of zero (gamma^0 = 1) and then in each\n",
    "    # step it increases by one (gamma^1 = gamma, gamma^2 = gamma^2, ...)\n",
    "    episode_gamma : float = 1\n",
    "\n",
    "    # Initialize a new game, we get back:\n",
    "    # - Description of the initial room\n",
    "    # - A description of the quest for this episode\n",
    "    # - A variable indicating if the game is done (Initially false to indicate that its starting)\n",
    "    (current_room_desc, current_quest_desc, terminal) = framework.newGame()\n",
    "\n",
    "    # If this is not the final episode\n",
    "    while not terminal:\n",
    "\n",
    "        # Get the state \"S\" by concatenating Sr (Room description) and Sq (Quest description)\n",
    "        current_state : str = current_room_desc + current_quest_desc\n",
    "\n",
    "        # Get a smaller dimensional representation of the current state\n",
    "        # by using the bag of words encoding strategy\n",
    "        current_state_vector : np.ndarray = utils.extract_bow_feature_vector(current_state, dictionary)\n",
    "\n",
    "        # Choose an action using the epsilon-greedy method\n",
    "        action_index, object_index = epsilon_greedy(current_state_vector, theta, epsilon)\n",
    "\n",
    "         # Run a step of the game. This returns:\n",
    "        # - Future room description (S'r)\n",
    "        # - Future quest description (S'q)\n",
    "        # - Reward (R)\n",
    "        # - Terminal: Whether the game has finished or not\n",
    "        next_room_desc, next_quest_desc, reward, terminal = framework.step_game(\n",
    "            current_room_desc, \n",
    "            current_quest_desc, \n",
    "            action_index, \n",
    "            object_index\n",
    "        )\n",
    "\n",
    "        # Get the state \"S'\" by concatenating the future room and quest descriptions\n",
    "        next_state : str = next_room_desc + next_quest_desc\n",
    "\n",
    "        # Create the \"next_state_vector\" using the bag of words encoding strategy\n",
    "        next_state_vector : np.ndarray = utils.extract_bow_feature_vector(next_state, dictionary)\n",
    "\n",
    "        if for_training:\n",
    "            linear_q_learning(\n",
    "                theta, \n",
    "                current_state_vector, \n",
    "                action_index, object_index, \n",
    "                reward, \n",
    "                next_state_vector, \n",
    "                terminal\n",
    "            )\n",
    "\n",
    "        if not for_training:\n",
    "            \n",
    "            # Calculate the discounted reward\n",
    "            episode_reward += reward * episode_gamma\n",
    "\n",
    "            # Increase the power of the gamma used for next episode\n",
    "            episode_gamma *= GAMMA\n",
    "\n",
    "\n",
    "        # The current \"next_step\" will consist of the \"current_step\" in the\n",
    "        # next pass of the while loop\n",
    "        current_room_desc = next_room_desc\n",
    "        current_quest_desc = next_quest_desc\n",
    "\n",
    "    if not for_training:\n",
    "        return episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the scripts present inside of the external scripts related to this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rl.utils' from 'e:\\\\Archivos\\\\Educación\\\\Posgrado\\\\Data Science (Universidad Galileo)\\\\2022 (MITx)\\\\Machine Learning with Python\\\\Unit 5\\\\Project 5\\\\rl\\\\utils.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl import agent_linear, framework, utils\n",
    "\n",
    "import importlib\n",
    "importlib.reload(agent_linear)\n",
    "importlib.reload(framework)\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the actual algorithm and take note of the average episodic rewards of your Q-learning algorithm when it converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/600 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 34 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\Archivos\\Educación\\Posgrado\\Data Science (Universidad Galileo)\\2022 (MITx)\\Machine Learning with Python\\Unit 5\\Project 5\\Project 5 - Q Learning.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Archivos/Educaci%C3%B3n/Posgrado/Data%20Science%20%28Universidad%20Galileo%29/2022%20%28MITx%29/Machine%20Learning%20with%20Python/Unit%205/Project%205/Project%205%20-%20Q%20Learning.ipynb#X32sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Run the linear agent. This includes training said agent and then returning \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Archivos/Educaci%C3%B3n/Posgrado/Data%20Science%20%28Universidad%20Galileo%29/2022%20%28MITx%29/Machine%20Learning%20with%20Python/Unit%205/Project%205/Project%205%20-%20Q%20Learning.ipynb#X32sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# the cumulative reward after testing\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Archivos/Educaci%C3%B3n/Posgrado/Data%20Science%20%28Universidad%20Galileo%29/2022%20%28MITx%29/Machine%20Learning%20with%20Python/Unit%205/Project%205/Project%205%20-%20Q%20Learning.ipynb#X32sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_RUNS):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Archivos/Educaci%C3%B3n/Posgrado/Data%20Science%20%28Universidad%20Galileo%29/2022%20%28MITx%29/Machine%20Learning%20with%20Python/Unit%205/Project%205/Project%205%20-%20Q%20Learning.ipynb#X32sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     epoch_rewards_test\u001b[39m.\u001b[39mappend(run(action_dim, state_dim, dictionary))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Archivos/Educaci%C3%B3n/Posgrado/Data%20Science%20%28Universidad%20Galileo%29/2022%20%28MITx%29/Machine%20Learning%20with%20Python/Unit%205/Project%205/Project%205%20-%20Q%20Learning.ipynb#X32sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# We convert the epoch rewards list into a numpy array\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Archivos/Educaci%C3%B3n/Posgrado/Data%20Science%20%28Universidad%20Galileo%29/2022%20%28MITx%29/Machine%20Learning%20with%20Python/Unit%205/Project%205/Project%205%20-%20Q%20Learning.ipynb#X32sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m epoch_rewards_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(epoch_rewards_test)\n",
      "File \u001b[1;32me:\\Archivos\\Educación\\Posgrado\\Data Science (Universidad Galileo)\\2022 (MITx)\\Machine Learning with Python\\Unit 5\\Project 5\\rl\\agent_linear.py:303\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(action_dim, state_dim, dict)\u001b[0m\n\u001b[0;32m    301\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(\u001b[39mrange\u001b[39m(NUM_EPOCHS), ncols\u001b[39m=\u001b[39m\u001b[39m80\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m--> 303\u001b[0m     single_run_epoch_rewards_test\u001b[39m.\u001b[39mappend(run_epoch())\n\u001b[0;32m    304\u001b[0m     pbar\u001b[39m.\u001b[39mset_description(\n\u001b[0;32m    305\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAvg reward: \u001b[39m\u001b[39m{:0.6f}\u001b[39;00m\u001b[39m | Ewma reward: \u001b[39m\u001b[39m{:0.6f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    306\u001b[0m             np\u001b[39m.\u001b[39mmean(single_run_epoch_rewards_test),\n\u001b[0;32m    307\u001b[0m             utils\u001b[39m.\u001b[39mewma(single_run_epoch_rewards_test)))\n\u001b[0;32m    308\u001b[0m \u001b[39mreturn\u001b[39;00m single_run_epoch_rewards_test\n",
      "File \u001b[1;32me:\\Archivos\\Educación\\Posgrado\\Data Science (Universidad Galileo)\\2022 (MITx)\\Machine Learning with Python\\Unit 5\\Project 5\\rl\\agent_linear.py:281\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m    278\u001b[0m rewards \u001b[39m=\u001b[39m []\n\u001b[0;32m    280\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_EPIS_TRAIN):\n\u001b[1;32m--> 281\u001b[0m     run_episode(for_training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    283\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_EPIS_TEST):\n\u001b[0;32m    284\u001b[0m     rewards\u001b[39m.\u001b[39mappend(run_episode(for_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "File \u001b[1;32me:\\Archivos\\Educación\\Posgrado\\Data Science (Universidad Galileo)\\2022 (MITx)\\Machine Learning with Python\\Unit 5\\Project 5\\rl\\agent_linear.py:246\u001b[0m, in \u001b[0;36mrun_episode\u001b[1;34m(for_training)\u001b[0m\n\u001b[0;32m    243\u001b[0m next_state_vector : np\u001b[39m.\u001b[39mndarray \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mextract_bow_feature_vector(next_state, dictionary)\n\u001b[0;32m    245\u001b[0m \u001b[39mif\u001b[39;00m for_training:\n\u001b[1;32m--> 246\u001b[0m     linear_q_learning(\n\u001b[0;32m    247\u001b[0m         theta, \n\u001b[0;32m    248\u001b[0m         current_state_vector, \n\u001b[0;32m    249\u001b[0m         action_index, object_index, \n\u001b[0;32m    250\u001b[0m         reward, \n\u001b[0;32m    251\u001b[0m         next_state_vector, \n\u001b[0;32m    252\u001b[0m         terminal\n\u001b[0;32m    253\u001b[0m     )\n\u001b[0;32m    255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m for_training:\n\u001b[0;32m    256\u001b[0m     \n\u001b[0;32m    257\u001b[0m     \u001b[39m# Calculate the discounted reward\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     episode_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward \u001b[39m*\u001b[39m episode_gamma\n",
      "File \u001b[1;32me:\\Archivos\\Educación\\Posgrado\\Data Science (Universidad Galileo)\\2022 (MITx)\\Machine Learning with Python\\Unit 5\\Project 5\\rl\\agent_linear.py:150\u001b[0m, in \u001b[0;36mlinear_q_learning\u001b[1;34m(theta, current_state_vector, action_index, object_index, reward, next_state_vector, terminal)\u001b[0m\n\u001b[0;32m    147\u001b[0m c \u001b[39m=\u001b[39m tuple2index(action_index, object_index)\n\u001b[0;32m    149\u001b[0m \u001b[39m# Select the Q function values that correspond to the current action C (a,b)\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m q_value_c : \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m q_values_current[c]\n\u001b[0;32m    152\u001b[0m \u001b[39m# NOTE:\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[39m# Now that the Q values are generated using a vector representation\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39m# of state in conjunction with a value matrix, we dont actually need to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m \n\u001b[0;32m    166\u001b[0m \u001b[39m# Target value \"y\" or the sampled version of the bellman operator\u001b[39;00m\n\u001b[0;32m    167\u001b[0m y : \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m (GAMMA \u001b[39m*\u001b[39m max_Q_next)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 34 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "from rl.agent_linear import run\n",
    "\n",
    "\n",
    "# Load the game states (Rooms and quest descriptions)\n",
    "state_texts = utils.load_data('./rl/game.tsv')\n",
    "dictionary = utils.bag_of_words(state_texts)\n",
    "\n",
    "# We get the number of possible states\n",
    "state_dim = len(dictionary)\n",
    "\n",
    "# We get the number of possible actions\n",
    "action_dim = NUM_ACTIONS * NUM_OBJECTS\n",
    "\n",
    "# Set up the game\n",
    "framework.load_game_data()\n",
    "\n",
    "# Initialize epoch rewards list\n",
    "# Shape: NUM_RUNS * NUM_EPOCHS\n",
    "epoch_rewards_test = []  \n",
    "\n",
    "# Run the linear agent. This includes training said agent and then returning \n",
    "# the cumulative reward after testing\n",
    "for _ in range(NUM_RUNS):\n",
    "    epoch_rewards_test.append(run(action_dim, state_dim, dictionary))\n",
    "\n",
    "# We convert the epoch rewards list into a numpy array\n",
    "epoch_rewards_test = np.array(epoch_rewards_test)\n",
    "\n",
    "# ============================================== #\n",
    "# PLOT                                           #\n",
    "# ============================================== #\n",
    "\n",
    "# X-axis: One tick per epoch\n",
    "x = np.arange(NUM_EPOCHS)\n",
    "\n",
    "# New figure\n",
    "fig, axis = plt.subplots()\n",
    "\n",
    "# Plot reward per epoch averaged per run\n",
    "axis.plot(x, np.mean(epoch_rewards_test, axis=0))  \n",
    "axis.set_xlabel('Epochs')\n",
    "axis.set_ylabel('reward')\n",
    "axis.set_title(f'Linear: nRuns={NUM_RUNS}, Epsilon={TRAINING_EP}, Epi={NUM_EPIS_TRAIN}, alpha={ALPHA}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('Machine_Learning_with_Python-7V858pgh')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbf55fd141a929e96d217693ff943f340da010ee827d3bb7b01df8ee1539f1d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
